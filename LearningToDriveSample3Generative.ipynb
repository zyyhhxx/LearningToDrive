{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started:\n",
    "## A simple driving model training and evaluation pipeline using the Drive360 dataset and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from Drive360 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dataset.py** file contains the 3 classes necessary for creating a Drive360Loader. Using the **config.json** file to specify the location of the csv and data directory, we can generate phase (train, validation, test) specific data loaders that can output samples from each set. Adjust the **dataset.py** to your preferred training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train # of data: 159568\n",
      "Phase: validation # of data: 10221\n",
      "Phase: test # of data: 27920\n",
      "Loaded train loader with the following data available as a dict.\n",
      "Index(['cameraRight', 'cameraFront', 'cameraRear', 'cameraLeft', 'here',\n",
      "       'tomtom', 'gpsLatitude', 'gpsLongitude', 'gpsAltitude', 'gpsPrecision',\n",
      "       'hereMmLatitude', 'hereMmLongitude', 'hereSpeedLimit',\n",
      "       'hereSpeedLimit_2', 'hereFreeFlowSpeed', 'hereSignal', 'hereYield',\n",
      "       'herePedestrian', 'hereIntersection', 'hereMmIntersection',\n",
      "       'hereSegmentExitHeading', 'hereSegmentEntryHeading',\n",
      "       'hereSegmentOthersHeading', 'hereCurvature', 'hereCurrentHeading',\n",
      "       'here1mHeading', 'here5mHeading', 'here10mHeading', 'here20mHeading',\n",
      "       'here50mHeading', 'hereTurnNumber', 'canSteering', 'canSpeed',\n",
      "       'chapter'],\n",
      "      dtype='object')\n",
      "Ready to train on 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from datasetfull import Drive360Loader\n",
    "\n",
    "# load the config.json file that specifies data \n",
    "# location parameters and other hyperparameters \n",
    "# required.\n",
    "config = json.load(open('./config_sample1_downsampled.json'))\n",
    "\n",
    "# create a train, validation and test data loader\n",
    "train_loader = Drive360Loader(config, 'train')\n",
    "validation_loader = Drive360Loader(config, 'validation')\n",
    "test_loader = Drive360Loader(config, 'test')\n",
    "\n",
    "# print the data (keys) available for use. See full \n",
    "# description of each data type in the documents.\n",
    "print('Loaded train loader with the following data available as a dict.')\n",
    "print(train_loader.drive360.dataframe.keys())\n",
    "\n",
    "total_batch = len(train_loader)\n",
    "print(\"Ready to train on {}\".format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = printEnd)\n",
    "    # Print New Line on Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a basic driving model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your driving model. This is specific to your learning framework. \n",
    "\n",
    "Below we give a very basic dummy model that uses the front facing camera and a resnet34 + LSTM architecture to predict canSteering and canSpeed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "    \n",
    "class GenerativeDefaultModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenerativeDefaultModel, self).__init__()\n",
    "        # For number inputs\n",
    "        self.number_labels = ['hereSpeedLimit',\n",
    "                                'hereFreeFlowSpeed', 'hereSignal', 'hereYield',\n",
    "                               'herePedestrian', 'hereIntersection', 'hereMmIntersection',\n",
    "                               'hereSegmentExitHeading', 'hereSegmentEntryHeading',\n",
    "                               'hereCurvature', \n",
    "                               'here1mHeading', 'here5mHeading', 'here10mHeading', 'here20mHeading',\n",
    "                               'here50mHeading']\n",
    "        final_concat_size = len(self.number_labels)\n",
    "        \n",
    "        # CNN Front\n",
    "        cnn_front = models.resnet50(pretrained=True)\n",
    "        self.front_features = nn.Sequential(*list(cnn_front.children())[:-1])\n",
    "        self.front_intermediate = nn.Sequential(\n",
    "            nn.Linear(cnn_front.fc.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU())\n",
    "        final_concat_size += 128\n",
    "\n",
    "        # Main LSTM\n",
    "        self.front_lstm = nn.LSTM(input_size=128,\n",
    "            hidden_size=64,\n",
    "            num_layers=3,\n",
    "            batch_first=False)\n",
    "        final_concat_size += 64\n",
    "        \n",
    "        # Angle Regressor\n",
    "        self.control_angle = nn.Sequential(\n",
    "            nn.Linear(final_concat_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        # Speed Regressor\n",
    "        self.control_speed = nn.Sequential(\n",
    "            nn.Linear(final_concat_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        module_outputs = []\n",
    "        lstm_i = []\n",
    "        \n",
    "        # Number parameters\n",
    "        for numbers in self.number_labels:\n",
    "            module_outputs.append(data[numbers].float().cuda())\n",
    "            \n",
    "        #Tomtom maps\n",
    "        #module_outputs.append(data['tomtom'].cuda())\n",
    "        \n",
    "        # Loop through temporal sequence of\n",
    "        # front facing camera images and pass \n",
    "        # through the cnn.\n",
    "        for k, v in data['cameraFront'].items():\n",
    "            v = v.cuda()\n",
    "            x = self.front_features(v)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.front_intermediate(x)\n",
    "            lstm_i.append(x)\n",
    "            # feed the current front facing camera\n",
    "            # output directly into the \n",
    "            # regression networks.\n",
    "            if k == 0:\n",
    "                module_outputs.append(x)\n",
    "\n",
    "        # Feed temporal outputs of CNN into LSTM\n",
    "        i_lstm, _ = self.front_lstm(torch.stack(lstm_i))\n",
    "        module_outputs.append(i_lstm[-1])\n",
    "        \n",
    "        # Concatenate current image CNN output \n",
    "        # and LSTM output.\n",
    "        x_cat = torch.cat(module_outputs, dim=-1)\n",
    "        \n",
    "        # Feed concatenated outputs into the \n",
    "        # regession networks.\n",
    "        prediction = {'canSteering': torch.squeeze(self.control_angle(x_cat)),\n",
    "                      'canSpeed': torch.squeeze(self.control_speed(x_cat))}\n",
    "        return prediction\n",
    "    \n",
    "class GenerativeDropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenerativeDropoutModel, self).__init__()\n",
    "        # For number inputs\n",
    "        self.number_labels = ['hereSpeedLimit',\n",
    "                                'hereFreeFlowSpeed', 'hereSignal', 'hereYield',\n",
    "                               'herePedestrian', 'hereIntersection', 'hereMmIntersection',\n",
    "                               'hereSegmentExitHeading', 'hereSegmentEntryHeading',\n",
    "                               'hereCurvature', \n",
    "                               'here1mHeading', 'here5mHeading', 'here10mHeading', 'here20mHeading',\n",
    "                               'here50mHeading']\n",
    "        final_concat_size = len(self.number_labels)\n",
    "        \n",
    "        # CNN Front\n",
    "        cnn_front = models.resnet50(pretrained=True)\n",
    "        self.front_features = nn.Sequential(*list(cnn_front.children())[:-1])\n",
    "        self.front_intermediate = nn.Sequential(\n",
    "            nn.Linear(cnn_front.fc.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        final_concat_size += 128\n",
    "\n",
    "        # Main LSTM\n",
    "        self.front_lstm = nn.LSTM(input_size=128,\n",
    "            hidden_size=64,\n",
    "            num_layers=3,\n",
    "            batch_first=False)\n",
    "        final_concat_size += 64\n",
    "        \n",
    "        # Angle Regressor\n",
    "        self.control_angle = nn.Sequential(\n",
    "            nn.Linear(final_concat_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        # Speed Regressor\n",
    "        self.control_speed = nn.Sequential(\n",
    "            nn.Linear(final_concat_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        module_outputs = []\n",
    "        lstm_i = []\n",
    "        \n",
    "        # Number parameters\n",
    "        for numbers in self.number_labels:\n",
    "            module_outputs.append(data[numbers].float().cuda())\n",
    "            \n",
    "        #Tomtom maps\n",
    "        #module_outputs.append(data['tomtom'].cuda())\n",
    "        \n",
    "        # Loop through temporal sequence of\n",
    "        # front facing camera images and pass \n",
    "        # through the cnn.\n",
    "        for k, v in data['cameraFront'].items():\n",
    "            v = v.cuda()\n",
    "            x = self.front_features(v)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.front_intermediate(x)\n",
    "            lstm_i.append(x)\n",
    "            # feed the current front facing camera\n",
    "            # output directly into the \n",
    "            # regression networks.\n",
    "            if k == 0:\n",
    "                module_outputs.append(x)\n",
    "\n",
    "        # Feed temporal outputs of CNN into LSTM\n",
    "        i_lstm, _ = self.front_lstm(torch.stack(lstm_i))\n",
    "        module_outputs.append(i_lstm[-1])\n",
    "        \n",
    "        # Concatenate current image CNN output \n",
    "        # and LSTM output.\n",
    "        x_cat = torch.cat(module_outputs, dim=-1)\n",
    "        \n",
    "        # Feed concatenated outputs into the \n",
    "        # regession networks.\n",
    "        prediction = {'canSteering': torch.squeeze(self.control_angle(x_cat)),\n",
    "                      'canSpeed': torch.squeeze(self.control_speed(x_cat))}\n",
    "        return prediction\n",
    "    \n",
    "class GenerativeDropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenerativeDropoutModel, self).__init__()\n",
    "        # For number inputs\n",
    "        self.number_labels = ['hereSpeedLimit',\n",
    "                                'hereFreeFlowSpeed', 'hereSignal', 'hereYield',\n",
    "                               'herePedestrian', 'hereIntersection', 'hereMmIntersection',\n",
    "                               'hereSegmentExitHeading', 'hereSegmentEntryHeading',\n",
    "                               'hereCurvature', \n",
    "                               'here1mHeading', 'here5mHeading', 'here10mHeading', 'here20mHeading',\n",
    "                               'here50mHeading']\n",
    "        final_concat_size = len(self.number_labels)\n",
    "        \n",
    "        # CNN Front\n",
    "        cnn_front = models.resnet50(pretrained=True)\n",
    "        self.front_features = nn.Sequential(*list(cnn_front.children())[:-1])\n",
    "        self.front_intermediate = nn.Sequential(\n",
    "            nn.Linear(cnn_front.fc.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        final_concat_size += 128\n",
    "\n",
    "        # Main LSTM\n",
    "        self.front_lstm = nn.LSTM(input_size=128,\n",
    "            hidden_size=64,\n",
    "            num_layers=3,\n",
    "            batch_first=False)\n",
    "        final_concat_size += 64\n",
    "        \n",
    "        # CNN Tomtom\n",
    "#        cnn_tomtom = models.resnet34(pretrained=True)\n",
    "#        self.tomtom_features = nn.Sequential(*list(cnn_tomtom.children())[:-1])\n",
    "#        self.tomtom_intermediate = nn.Sequential(nn.Linear(\n",
    "#                          cnn_tomtom.fc.in_features, 128),\n",
    "#                          nn.ReLU())\n",
    "#        final_concat_size += 128\n",
    "        \n",
    "        # CNN HERE\n",
    "        cnn_here = models.resnet34(pretrained=True)\n",
    "        self.here_features = nn.Sequential(*list(cnn_here.children())[:-1])\n",
    "        self.here_intermediate = nn.Sequential(nn.Linear(\n",
    "                          cnn_here.fc.in_features, 128),\n",
    "                          nn.ReLU())\n",
    "        final_concat_size += 128\n",
    "        \n",
    "        # Angle Regressor\n",
    "        self.control_angle = nn.Sequential(\n",
    "            nn.Linear(final_concat_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        # Speed Regressor\n",
    "        self.control_speed = nn.Sequential(\n",
    "            nn.Linear(final_concat_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        module_outputs = []\n",
    "        lstm_i = []\n",
    "        \n",
    "        # Number parameters\n",
    "        for numbers in self.number_labels:\n",
    "            module_outputs.append(data[numbers].float().cuda())\n",
    "            \n",
    "        # HERE map\n",
    "        here = self.here_features(data['here'])\n",
    "        here = here.view(here.size(0), -1)\n",
    "        here = self.front_intermediate(here)\n",
    "        module_outputs.append(here)\n",
    "\n",
    "        #Tomtom maps\n",
    "        #module_outputs.append(data['tomtom'].cuda())\n",
    "        \n",
    "        # Loop through temporal sequence of\n",
    "        # front facing camera images and pass \n",
    "        # through the cnn.\n",
    "        for k, v in data['cameraFront'].items():\n",
    "            v = v.cuda()\n",
    "            x = self.front_features(v)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.front_intermediate(x)\n",
    "            lstm_i.append(x)\n",
    "            # feed the current front facing camera\n",
    "            # output directly into the \n",
    "            # regression networks.\n",
    "            if k == 0:\n",
    "                module_outputs.append(x)\n",
    "\n",
    "        # Feed temporal outputs of CNN into LSTM\n",
    "        i_lstm, _ = self.front_lstm(torch.stack(lstm_i))\n",
    "        module_outputs.append(i_lstm[-1])\n",
    "        \n",
    "        # Concatenate current image CNN output \n",
    "        # and LSTM output.\n",
    "        x_cat = torch.cat(module_outputs, dim=-1)\n",
    "        \n",
    "        # Feed concatenated outputs into the \n",
    "        # regession networks.\n",
    "        prediction = {'canSteering': torch.squeeze(self.control_angle(x_cat)),\n",
    "                      'canSpeed': torch.squeeze(self.control_speed(x_cat))}\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic training procedure that iterates over the train_loader and feeds each sample into our dummy model, subsequently calculates loss. We kill after 20 batches just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, loss, path):\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, path)\n",
    "    \n",
    "def resume_model(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    model.cuda()\n",
    "    #optimizer.cuda()\n",
    "    \n",
    "    return epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "\n",
    "def train_model(model, optimizer, criterion_speed, criterion_steering, epochs, scheduler, name, if_test, resume_path=None):\n",
    "    hist_train_loss_speed = []\n",
    "    hist_train_loss_steering = []\n",
    "    hist_valid_loss_speed = []\n",
    "    hist_valid_loss_steering = []\n",
    "    \n",
    "    model.cuda()\n",
    "    best_model = None\n",
    "    best_valid = math.inf\n",
    "    \n",
    "    if resume_path:\n",
    "        epoch, _ = resume_model(model, optimizer, resume_path)\n",
    "    else:\n",
    "        epoch = 0\n",
    "    \n",
    "    while epoch < epochs:\n",
    "        start = time.time()\n",
    "        printProgressBar(0, total_batch, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "        model.train()\n",
    "        progress_counter = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(data)\n",
    "\n",
    "            steering_loss = criterion_steering(prediction['canSteering'].cuda(), target['canSteering'].cuda())\n",
    "            speed_loss = criterion_speed(prediction['canSpeed'].cuda(), target['canSpeed'].cuda())\n",
    "            loss = steering_loss + speed_loss\n",
    "            hist_train_loss_speed.append(speed_loss.cpu().detach().numpy())\n",
    "            hist_train_loss_steering.append(steering_loss.cpu().detach().numpy())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "                \n",
    "            progress_counter += 1\n",
    "            if progress_counter >= 10:\n",
    "                printProgressBar(batch_idx + 1, total_batch, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "                progress_counter = 0\n",
    "        printProgressBar(total_batch, total_batch, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "        print()\n",
    "        \n",
    "        end = time.time() \n",
    "        print(\"Training minutes elapsed epoch{}: {}, {} left\".format(epoch, round((end - start) / 60, 2), round((end - start) * (epochs - epoch - 1) / 60, 2)))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        speed_sum = 0\n",
    "        steering_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "                optimizer.zero_grad()\n",
    "                prediction = model(data)\n",
    "                speed_dif = prediction['canSpeed'].cpu() - target['canSpeed']\n",
    "                steering_dif = prediction['canSteering'].cpu() - target['canSteering']\n",
    "                speed_sum += (np.square(speed_dif)).mean()\n",
    "                steering_sum += (np.square(steering_dif)).mean()\n",
    "        hist_valid_loss_speed.append(speed_sum)\n",
    "        hist_valid_loss_steering.append(steering_sum)\n",
    "        print(\"Steering Error: {}\\nSpeed Error: {}\".format(float(steering_sum), float(speed_sum)))\n",
    "        \n",
    "        # Early Stopping\n",
    "        if steering_sum < best_valid:\n",
    "            best_model = deepcopy(model)\n",
    "            best_valid = steering_sum\n",
    "            print(\"Find a new best model!\")\n",
    "        \n",
    "        epoch += 1\n",
    "    \n",
    "    plt.plot(hist_train_loss_speed)\n",
    "    plt.title(\"Speed Loss\")\n",
    "    now = datetime.now()\n",
    "    plt.plot(hist_train_loss_speed)\n",
    "    plt.title(\"Speed Loss\")\n",
    "    if not if_test:\n",
    "        plt.savefig(\"./saved_figures/{}-{}-{}-speed.png\".format(epochs, name, now.strftime(\"%m-%d-%Y-%H-%M-%S\")))\n",
    "    plt.show()\n",
    "    plt.plot(hist_train_loss_steering)\n",
    "    plt.title(\"Steering Loss\")\n",
    "    if not if_test:\n",
    "        plt.savefig(\"./saved_figures/{}-{}-{}-angle.png\".format(epochs, name, now.strftime(\"%m-%d-%Y-%H-%M-%S\")))\n",
    "    plt.show()\n",
    "    if not if_test:\n",
    "        save_model(model, optimizer, epoch, loss, \n",
    "                   \"./saved_models/{}-{}-{}.pt\".format(epochs, name, now.strftime(\"%m-%d-%Y-%H-%M-%S\")))\n",
    "        print(\"Saving current_model to \" + \"./saved_models/{}-{}-{}.pt\".format(epochs, name, now.strftime(\"%m-%d-%Y-%H-%M-%S\")))\n",
    "        save_model(model, optimizer, epoch, loss, \n",
    "                   \"./best_models/{}-{}-{}.pt\".format(epochs, name, now.strftime(\"%m-%d-%Y-%H-%M-%S\")))\n",
    "    plt.plot(hist_valid_loss_speed)\n",
    "    plt.title(\"Validating Speed Loss\")\n",
    "    plt.show()\n",
    "    plt.plot(hist_valid_loss_steering)\n",
    "    plt.title(\"Validating Steering Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test data: 279863\n"
     ]
    }
   ],
   "source": [
    "normalize_targets = config['target']['normalize']\n",
    "target_mean = config['target']['mean']\n",
    "target_std = config['target']['std']\n",
    "\n",
    "def add_results(results, output):\n",
    "    steering = np.squeeze(output['canSteering'].cpu().data.numpy())\n",
    "    speed = np.squeeze(output['canSpeed'].cpu().data.numpy())\n",
    "    if normalize_targets:\n",
    "        steering = (steering*target_std['canSteering'])+target_mean['canSteering']\n",
    "        speed = (speed*target_std['canSpeed'])+target_mean['canSpeed']\n",
    "    if np.isscalar(steering):\n",
    "        steering = [steering]\n",
    "    if np.isscalar(speed):\n",
    "        speed = [speed]\n",
    "    results['canSteering'].extend(steering)\n",
    "    results['canSpeed'].extend(speed)\n",
    "\n",
    "test_chapter_length = [2997 for i in range(98)]\n",
    "test_chapter_length[14] = 2996\n",
    "test_chapter_length[36] = 2996\n",
    "test_chapter_length[38] = 2996\n",
    "test_chapter_length[63] = 1277\n",
    "test_chapter_length[64] = 2861\n",
    "test_chapter_length[66] = 2014\n",
    "test_chapter_length[70] = 2996\n",
    "test_chapter_length[72] = 2690\n",
    "test_chapter_length[81] = 2106\n",
    "test_chapter_length[85] = 2996\n",
    "test_chapter_length[92] = 2996\n",
    "\n",
    "for i in range(len(test_chapter_length)):\n",
    "    test_chapter_length[i] -= 100\n",
    "    \n",
    "print(\"Number of test data: {}\".format(sum(test_chapter_length)))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def create_submission(model_index, initial_offset, interval):\n",
    "    now = datetime.now()\n",
    "    file_name = \"./submissions/{}-{}.csv\".format(names[model_index], now.strftime(\"%m-%d-%Y-%H-%M-%S\"))\n",
    "    results = {'canSteering': [],\n",
    "               'canSpeed': []}\n",
    "\n",
    "    final_result = {'canSteering': np.zeros(279863, dtype=\"float64\"),\n",
    "                    'canSpeed': np.zeros(279863, dtype=\"float64\")}\n",
    "    final_result['canSteering'][:] = np.nan\n",
    "    final_result['canSpeed'][:] = np.nan\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"Start creating submission file {}, offset:{}, interval:{}\".format(file_name, initial_offset, interval))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            prediction = user_models[model_index](data)\n",
    "            add_results(results, prediction)\n",
    "    \n",
    "    index = 0\n",
    "    chapter_cap = 0\n",
    "    r_index = 0\n",
    "    for chapter_num in test_chapter_length:\n",
    "        index = chapter_cap + initial_offset\n",
    "        \n",
    "        #Fill the leading indices\n",
    "        final_result['canSteering'][chapter_cap] = results['canSteering'][r_index]\n",
    "        final_result['canSpeed'][chapter_cap] = results['canSpeed'][r_index]\n",
    "        \n",
    "        chapter_cap += chapter_num\n",
    "        while index < chapter_cap:\n",
    "            final_result['canSteering'][index] = results['canSteering'][r_index]\n",
    "            final_result['canSpeed'][index] = results['canSpeed'][r_index]\n",
    "            \n",
    "            index += interval\n",
    "            r_index += 1\n",
    "            \n",
    "        #Fill the trailing indices\n",
    "        #print(index, chapter_cap, r_index)\n",
    "        final_result['canSteering'][chapter_cap - 1] = results['canSteering'][r_index - 1]\n",
    "        final_result['canSpeed'][chapter_cap - 1] = results['canSpeed'][r_index - 1]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(final_result, dtype='float64')\n",
    "    df.interpolate(method='linear', limit_direction='forward', inplace=True)\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "    end = time.time() \n",
    "    print(\"Minutes elapsed: {}\".format(round((end - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(user_models, optimizers, names, index, epochs=1, if_test=False, scheduler=None, resume_path=None):\n",
    "    print(names[index] + \": \")\n",
    "    train_model(user_models[index], optimizers[index], nn.SmoothL1Loss(), nn.SmoothL1Loss(), epochs, scheduler, names[index], if_test, resume_path)\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    if not if_test:\n",
    "        create_submission(index, config[\"initial_offset\"], config[\"interval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "user_models = [GenerativeDropoutModel(), GenerativeDefaultModel(), \n",
    "               GenerativeDefaultModel(), GenerativeDefaultModel()]\n",
    "optimizers = [optim.AdamW(user_models[0].parameters(), lr=0.0001),\n",
    "             optim.AdamW(user_models[1].parameters(), lr=0.001),\n",
    "             optim.AdamW(user_models[2].parameters(), lr=0.0001),\n",
    "             optim.AdamW(user_models[3].parameters(), lr=0.0001)]\n",
    "names = [\"FullResnet50GenDropout 0.001\", \n",
    "        \"Resnet50Dropout-lr-0.001\",\n",
    "        \"Resnet50Dropout-lr-0.0001\",\n",
    "        \"Resnet50BatchNormDropout-lr-0.0001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullResnet50GenDropout 0.001: \n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Training minutes elapsed epoch0: 12.8, 243.21 left\n",
      "Steering Error: 28.05217933654785\n",
      "Speed Error: 18.528594970703125\n",
      "Find a new best model!\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Training minutes elapsed epoch1: 12.76, 229.73 left\n",
      "Steering Error: 22.33787727355957\n",
      "Speed Error: 14.883105278015137\n",
      "Find a new best model!\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Training minutes elapsed epoch2: 13.94, 236.92 left\n",
      "Steering Error: 24.090362548828125\n",
      "Speed Error: 15.156240463256836\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Training minutes elapsed epoch3: 12.77, 204.39 left\n",
      "Steering Error: 21.553848266601562\n",
      "Speed Error: 16.70309829711914\n",
      "Find a new best model!\n",
      "Progress: |█████---------------------------------------------| 11.2% Complete\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 74, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 74, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 74, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 74, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 53, in default_collate\n    storage = elem.storage()._new_shared(numel)\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\storage.py\", line 128, in _new_shared\n    return cls._new_using_filename(size)\nRuntimeError: Couldn't open shared file mapping: <torch_424_3713731634>, error code: <1455>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-71bb618c0eec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#cross_validation(user_models, optimizers, names, 1, 10, True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#cross_validation(user_models, optimizers, names, 2, 100)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-24e6151ddaff>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(user_models, optimizers, names, index, epochs, if_test, scheduler, resume_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mif_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresume_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\": \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSmoothL1Loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSmoothL1Loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mif_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresume_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--------------------------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mif_test\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-6e28695839db>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, criterion_speed, criterion_steering, epochs, scheduler, name, if_test, resume_path)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprogress_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[1;31m# Python 2 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 74, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 74, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 74, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 74, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 53, in default_collate\n    storage = elem.storage()._new_shared(numel)\n  File \"C:\\Users\\yz3645\\Anaconda3\\lib\\site-packages\\torch\\storage.py\", line 128, in _new_shared\n    return cls._new_using_filename(size)\nRuntimeError: Couldn't open shared file mapping: <torch_424_3713731634>, error code: <1455>\n"
     ]
    }
   ],
   "source": [
    "cross_validation(user_models, optimizers, names, 0, 10, False)\n",
    "\n",
    "#cross_validation(user_models, optimizers, names, 1, 10, True)\n",
    "\n",
    "#cross_validation(user_models, optimizers, names, 2, 100)\n",
    "\n",
    "#cross_validation(user_models, optimizers, criterion_speeds, criterion_steerings, names, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizers[0], base_lr=0.000001, max_lr=0.001, cycle_momentum=False)\n",
    "#cross_validation(user_models, optimizers, criterion_speeds, criterion_steerings, 0, \n",
    "#                 \"(Resnet50 Model, AdamW optimizer, SmoothL1Loss, Cyclic LR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pandas to create a submission file which is simply a 2-column csv with a canSteering and canSpeed prediction for each row in the **drive360_test.csv** a total of 305437 rows/predictions not including the header. See the **sample_submission.csv** file as an example.\n",
    "\n",
    "IMPORTANT: for the test phase indices will start 10s (100 samples) into each chapter this is to allow challenge participants to experiment with different temporal settings of data input. If challenge participants have a greater temporal length than 10s for each training sample, then they must write a custom function here. Please check out the **dataset.py** file for additional explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "#create_submission(1, config[\"initial_offset\"], config[\"interval\"])\n",
    "#create_submission(2, config[\"initial_offset\"], config[\"interval\"])\n",
    "#create_submission(3, config[\"initial_offset\"], config[\"interval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
